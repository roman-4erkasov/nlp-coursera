{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seq2seq_calculator.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"deqskeLVVnVy","colab_type":"code","outputId":"8a65224c-4eb5-442c-86b0-6cf449c2ebff","colab":{}},"source":["import random\n","import tensorflow as tf\n","\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/Users/roman/py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n","  return f(*args, **kwds)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_14g_XVEVnV4","colab_type":"code","colab":{}},"source":["DATASET_SIZE = 100000\n","OPERATORS = [\"+\", \"-\"]\n","MIN = 1\n","MAX = 9\n","START = '^'\n","END = '$'\n","PAD = '#'\n","SEQ_LEN = 15\n","VOCAB = '#^$+-1234567890'\n","SPECIAL_VOCAB = {START, END, PAD}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tGuwNaBIVnV8","colab_type":"code","colab":{}},"source":["def expression(operators, min_value, max_value, nobs):\n","    for _ in range(nobs):\n","        x = random.randint(a=min_value, b=max_value)\n","        y = random.randint(a=min_value, b=max_value)\n","        op = random.choice(operators)\n","        expr = f\"{x}{op}{y}\"\n","        result = eval(expr)\n","        yield expr, result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMHkUPzzVnV-","colab_type":"code","colab":{}},"source":["dfull = list(expression(OPERATORS, MIN, MAX, DATASET_SIZE))\n","dtrain, dtest = train_test_split(dfull, test_size=0.2)\n","\n","word2id = {symbol:i for i, symbol in enumerate(VOCAB)}\n","id2word = {i:symbol for symbol, i in word2id.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M79meOg_VnWA","colab_type":"code","colab":{}},"source":["def sentence2ids(sentence, mapping, sequence_len):\n","    \"\"\"\n","    sentence: expresion\n","    mapping: dictionary that maps expression to list of indicies\n","    sequence_len: united length for all sequencies\n","    \"\"\"\n","    n_pads = max(sequence_len - len(sentence) - 1, 0)\n","    id_seq = [\n","        mapping[x] for i,x in enumerate(sentence) if i<sequence_len-1\n","    ]+[mapping[END]]+[mapping[PAD]]*n_pads\n","    return id_seq, len(id_seq)\n","\n","def id2sentence(sequence, mapping):\n","    return \"\".join([\n","        mapping[x] for x in sequence\n","        if mapping[x] not in SPECIAL_VOCAB\n","    ])\n","\n","def generate_batches(samples, batch_size=64):\n","    X, Y = [], []\n","    for i, (x, y) in enumerate(samples, 1):\n","        X.append(x)\n","        Y.append(y)\n","        if i % batch_size == 0:\n","            yield X, Y\n","            X, Y = [], []\n","    if X and Y:\n","        yield X, Y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"amLgnV0oVnWD","colab_type":"code","colab":{}},"source":["class Seq2Seq:\n","    def __init__(self):\n","        self.input_batch = tf.placeholder(\n","            shape=[None, None], #[batch_size, sequence_len]\n","            dtype=tf.int32, \n","            name='input_batch'\n","        )\n","        self.input_batch_lengths = tf.placeholder(\n","            shape=[None], #[batch_size]\n","            dtype=tf.int32, \n","            name=\"input_batch_lengths\"\n","        )\n","        self.ground_truth = tf.placeholder(\n","            shape=[None], #[batch_size]\n","            dtype=tf.int32, \n","            name=\"ground_truth\"\n","        )\n","        self.ground_truth_lengths = tf.placeholder(\n","            shape=[None], #[batch_size]\n","            dtype=tf.int32, \n","            name=\"ground_truth_lengths\"\n","        )\n","        self.droputh_ph = tf.placeholder(\n","            shape = None, #scalar\n","            dtype=tf.float32,\n","            name=\"dropout\"\n","        ) \n","        self.learning_rate_ph = tf.placeholder(\n","            shape=None, #scalar\n","            dtype=tf.float32,\n","            name=\"learning_rate\"\n","        )\n","        \n","    def __create_embeddings(self, vocab_size, embeddings_size):\n","        init_embeddings = tf.random_uniform(\n","            (vocab_size, embeddings_size),\n","            -1.,\n","            1.\n","        )\n","        #[vocab_size, embedding_size]\n","        self.embeddings = tf.Variable(\n","            init_embeddings,\n","            name=\"embedding_matrix\",\n","            dtype=tf.float32\n","        )\n","        self.input_batch_embedded = tf.nn.embedding_lookup(\n","            params=self.embeddings,\n","            ids=self.input_batch\n","        )\n","        \n","    def __build_encoder(self, hidden_size):\n","        encoder_cell = tf.nn.rnn_cell.DropoutWrapper(\n","            cell=tf.nn.rnn_cell.GRUCell(num_units=hidden_size),\n","            input_keep_prob=self.droputh_ph,\n","            state_keep_prob=self.droputh_ph,\n","            output_keep_prob=self.droputh_ph\n","        )\n","        _, self.final_encoder_state = tf.nn.dynamic_rnn(\n","            cell=encoder_cell,\n","            inputs=self.input_batch_embedded,\n","            sequence_length=self.input_batch_lengths,\n","            dtype=tf.float32\n","        )\n","    \n","    def __biuld_decoder(\n","        self,\n","        hidden_size, \n","        vocab_size, \n","        max_iter, \n","        start_symbol_id, \n","        end_symbol_id\n","    ):\n","        def decode(helper, scope, reuse=None):\n","            with tf.variable_scope(scope, reuse=reuse):\n","                decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(\n","                    cell=tf.nn.rnn_cell.GRUCell(\n","                        num_units=hidden_size,\n","                        reuse=reuse\n","                    ),\n","                    output_size=vocab_size,\n","                    reuse=reuse\n","                )        \n","                decoder=tf.contrib.seq2seq.BasicDecoder(\n","                    cell=decoder_cell,\n","                    helper=helper,\n","                    initial_state=self.final_encoder_state\n","                )\n","                outputs,_,_ = tf.contrib.seq2seq.dynamic_decode(\n","                    decoder=decoder,\n","                    maximum_iterations=max_iter, \n","                    output_time_major=False, \n","                    impute_finished=True#not ignore final state\n","                )\n","                return outputs\n","        batch_size = tf.shape(self.input_batch)[0]\n","        start_tokens = tf.fill([batch_size], start_symbol_id)\n","        \n","        # convert to shape [batch_size, 1] (column-vector)\n","        start_tokens_col = tf.expand_dims(start_tokens, 1)\n","        \n","        #add start token to every input sequence\n","        ground_truth_as_input = tf.concat(\n","            [start_tokens_col, self.ground_truth], \n","            1 #concat rows\n","        )\n","        self.ground_truth_embedded = tf.nn.embedding_lookup(\n","            params=self.embeddings,\n","            ids=ground_truth_as_input\n","        )\n","        #interface for training decoders\n","        train_helper = tf.contrib.seq2seq.TrainingHelper(\n","            inputs=self.ground_truth_embedded,\n","            sequence_length=self.ground_truth_lengths\n","        )\n","        #interface for infer decoders\n","        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n","            embedding=self.embeddings,\n","            start_tokens=start_tokens,\n","            end_token=end_symbol_id\n","        )\n","        self.train_outputs = decode(train_helper, \"train\")\n","        self.infer_outputs = decode(train_helper, \"infer\", reuse=True)\n","        \n","    def __compute_loss(self):\n","        weights = tf.cast(\n","            tf.sequence_mask(self.input_batch_lengths), \n","            dtype=tf.float32\n","        )\n","        self.loss = tf.contrib.seq2seq.sequence_loss(\n","            logits=self.train_outputs.rnn_outputs,\n","            targets=self.ground_truth,\n","            weights=weights\n","        )\n","    \n","    def __perform_optimisation(self):\n","        self.train_op = tf.contrib.layers.optimize_loss(\n","            loss=self.loss,\n","            global_step=tf.train.global_step(),\n","            learning_rate=self.learning_rate_ph,\n","            optimizer=tf.train.AdagradOptimizer(\n","                learning_rate=self.learning_rate_ph\n","            ),\n","            clip_gradients=1.\n","        )\n","        \n","    def init_model(\n","        self,\n","        vocab_size, \n","        embeddings_size, \n","        hidden_size, \n","        max_iter, \n","        start_symbol_id, \n","        end_symbol_id, \n","        padding_symbol_id\n","    ):\n","        self.__create_embeddings(vocab_size, embeddings_size)\n","        self.__build_encoder(hidden_size)\n","        self.__biuld_decoder(\n","            hidden_size, \n","            vocab_size, \n","            max_iter, \n","            start_symbol_id, \n","            end_symbol_id\n","        )\n","        self.__compute_loss()\n","        self.__perform_optimisation()\n","        \n","        self.train_predicion = self.train_outputs.sample_id\n","        self.infer_prediction = self.infer_outputs.sample_id\n","        \n","    def train_on_batch(\n","        self, \n","        session, \n","        X, X_seq_len, \n","        Y, Y_seq_len, \n","        learning_rate, \n","        dropout_keep_probability\n","    ):\n","        feed_dict = {\n","            self.input_batch: X,\n","            self.input_batch_lengths: X_seq_len,\n","            self.ground_truth: Y,\n","            self.ground_truth_lengths: Y_seq_len,\n","            self.droputh_ph: dropout_keep_probability,\n","            self.learning_rate_ph: learning_rate\n","        }\n","        pred, loss, _ = session.run(\n","            fetches=[self.train_predicion, self.loss, self.train_op],\n","            feed_dict=feed_dict\n","        )\n","        return pred, loss\n","    \n","    def predict_for_batch(\n","        self,\n","        session, \n","        X, \n","        X_seq_len\n","    ):\n","        feed_dict = {\n","            self.input_batch: X,\n","            self.input_batch_lengths: X_seq_len\n","        }\n","        pred = session.run(\n","            fetches=[self.infer_prediction],\n","            feed_dict=feed_dict\n","        )\n","        return pred\n","    \n","    def predict_for_batch_with_loss(\n","        self, \n","        session, \n","        X, X_seq_len, \n","        Y, Y_seq_len\n","    ):\n","        feed_dict = {\n","            self.input_batch: X,\n","            self.input_batch_lengths: X_seq_len,\n","            self.ground_truth: Y,\n","            self.ground_truth_lengths: Y_seq_len,\n","        }\n","        pred, loss = session.run(\n","            fetches=[self.infer_prediction, self.loss]\n","        )\n","        return pred, loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MClu5qRyVnWF","colab_type":"code","colab":{}},"source":["tf.reset_default_graph()\n","\n","model = Seq2SeqModel(\n","    vocab_size=len(word2id), \n","    embeddings_size=20, \n","    hidden_size=512, \n","    max_iter=7, \n","    start_symbol_id=word2id[\"^\"], \n","    end_symbol_id=word2id[\"$\"], \n","    padding_symbol_id=word2id[\"#\"]\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WswZECPtVnWH","colab_type":"code","outputId":"0cb492ca-76f8-43e2-8fa7-a57247f0829e","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1563127092237,"user_tz":-180,"elapsed":395,"user":{"displayName":"Fatvvs Fatvvs","photoUrl":"","userId":"11256666657313400608"}}},"source":["8400+4304"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12704"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"SYEoiDyMVnWM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}